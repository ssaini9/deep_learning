{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c97bda0-344f-4c01-9ad4-da015e07cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bc839-45e6-4436-ac7a-8ac77b1cce9e",
   "metadata": {},
   "source": [
    "pytorch tensors - \n",
    "1. Tensor - similar to array or matrix, building blocks of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f50f6d3-999e-49dc-a300-518ed3b30a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [[1,2,3],[4,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1812e0b-f478-4ffb-a866-3f17cf1cb0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cbd3d76-39ac-4001-ae4f-a3aec3c6809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6357e9-f592-4676-8053-c55b05710f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08fcb202-754f-466d-9553-2d268252ad17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6134a0-0abf-44bc-853e-408314308cde",
   "metadata": {},
   "source": [
    "compatible tensors - when their shapes align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3611d-293e-4ce5-a767-85ca66d7ab86",
   "metadata": {},
   "source": [
    "### Building Neural Network using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e54c04-b953-435c-8cd8-c73040d0a381",
   "metadata": {},
   "source": [
    "input layer - hidden layer - output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622a2cf-10e5-4bd4-957c-edfb3e44b0e0",
   "metadata": {},
   "source": [
    "### 1. First Neural Network \n",
    "- this does not have hidden layer\n",
    "- output layer is linear layer\n",
    "- every output neuron connects to every input neurons - **fully connected network**\n",
    "- this is equivalent to a linear model - helps us understand without adding complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a45cec92-bb08-46e0-9453-d0441d27718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d7f144-76a5-4d8c-ba09-0540106bbff7",
   "metadata": {},
   "source": [
    "when designing a neural network, the input and output layer dimensions are pre-defined. \n",
    "- input neurons = features\n",
    "- output neurons = classes (we want to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcf94cee-9bbb-4ebd-b8e8-99f2fa525ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input_tensor with three features - our input layer\n",
    "input_tensor = torch.tensor([[0.3471,0.4547,-0.2356]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59c85887-3b6f-4792-9835-54a10ac96341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our linear layer\n",
    "linear_layer = nn.Linear(\n",
    "    in_features =3,\n",
    "    out_features=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b31ff3db-eed3-4508-9ab0-3d70047d33a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5322,  0.1088]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pass input through linear layer\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60056714-10ec-4456-bbe6-ce53bb2a8357",
   "metadata": {},
   "source": [
    "when input_tensor is passed to linear_layer, a linear function is performed to include weights and biases, each linear_layer has sets of weights and biases - these are the key quantities that define a neuron\n",
    "- **weight** : reflects the importance of different features\n",
    "- **bias** : provides the neuron with a baseline output\n",
    "    - bias are independent of the weights\n",
    "- at first, linear layer assigns random weights and biases and these are tuned later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b6054a-7856-4ec0-8baa-5550e18134c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3334, -0.2678, -0.4806],\n",
      "        [ 0.0719, -0.2007, -0.2185]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e978265e-9930-4ecb-bcd7-89f162f9ccea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.4080,  0.1236], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb0f42-f2b9-49ea-a8e7-b55ee66a7472",
   "metadata": {},
   "source": [
    "**example** - let's say we have a weather dataset with three features - temperature, humidity and wind and we want to predict whether it's going to rain or be cloudy\n",
    "1. humidity feature will have more significant weight compared to other features as it is a strong predictor of whethers it's going to rain or not\n",
    "2. the data is for tropical region with high probability of rain, so a **bias** is added to account for this baseline information.\n",
    "- with these information our model makes a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d738790-cc05-4adb-b5e9-2edf4bfc9c86",
   "metadata": {},
   "source": [
    "### 2. Hidden Layers and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc7382-6054-4c13-89f4-882503eac225",
   "metadata": {},
   "source": [
    "- here we will add more layers to help the network learn complex patterns\n",
    "- stack three linear layers using nn.Sequential\n",
    "- nn.Squential is a pytorch container for stacking layers in sequence\n",
    "- takes input - passes it to each linear layers in sequence - returns output\n",
    "- layers within nn.Sequential are hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204ac2a-3698-4669-b629-3aeb60693396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network with three linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 8), #n_features represents number of input features\n",
    "    nn.Linear(8,4),\n",
    "    nn.Linear(4,n_classes) #n_classes represents number of output classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7544f4-cb4e-4fa4-9727-f5c0416a0e12",
   "metadata": {},
   "source": [
    "- we can keep adding as many layers as we want as long as the input dimension of first layers matches the output dimension of the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9872c-9ef1-4793-aefd-184184461eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding more layers  - three linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10,18), # takes 10 layers and output 18\n",
    "    nn.Linear(18,20), # takes 18 layers and output 20\n",
    "    nn.Linear(20,5) #Â takes 20 layers and output 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc3f2f-b501-479a-b651-ec26479f74c1",
   "metadata": {},
   "source": [
    "1. **layers are made of neurons**\n",
    "- a layer is fully connected when each neuron links to all neurons in the previous layer\n",
    "- a neuron in a linear layer :\n",
    "    - performs a linear operation using all neurons from the previous layer\n",
    "    - has n+1 parameters - n from inputs and 1 from the bias\n",
    "2. **Paramters and model capacity**\n",
    "- more hidden layer = more parameters = higher model capacity ( can handle complex dataset but may take longer to train)\n",
    "- an effective way to assess a models capacity is by calculating it's total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0fbb3-4f38-4259-8134-4d6d08cfbbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in 2 layer network \n",
    "model = nn.Sequential(nn.Linear(8,4),\n",
    "                      nn.Linear(4,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b320c38-0b1f-42bd-9b42-ece077c76732",
   "metadata": {},
   "source": [
    "**manual paramter calculation:**\n",
    "- first layer has 4 neurons, each neuron has 8+1 (8 weights and 1 bias) parameters. 9 times 4 = 36 parameters\n",
    "- second layer has 2 neurons, each neuron has 4+1 parameters. 5 times 2 = 10 parameters\n",
    "- in total this model has - 36 + 10 = 46 learnable parameters\n",
    "\n",
    "we can do this manual calculation in python using .numel() method\n",
    "- .numel() : returns the number of elements in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6009938-fc2a-4de0-8326-e00f592f3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for paramter in model.parameters():\n",
    "    total += paramater.numel()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ef3b6-186a-4e85-b6d7-8c417ed70f5c",
   "metadata": {},
   "source": [
    "understanding parameter count helps us understand model complexity and efficiency\n",
    "- too many parameters can lead to long training times or overfitting\n",
    "- too few parameters might limit learning capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33585ecf-1486-45d1-bb84-16a681190dfd",
   "metadata": {},
   "source": [
    "### 3. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ee9b6-270d-43ba-9fb2-fb4fb8a4e3d6",
   "metadata": {},
   "source": [
    "- **activation functions** add non-linearity to the network\n",
    "    - sigmoid for binary classification\n",
    "    - softmax for multi-class classification\n",
    "- this non-linearity allows networks to learn more complex interactions between inputs and targets than only linear relationship\n",
    "- **pre-activation** output passed to the activation function\n",
    "    - we will call the output of last linear layer the **pre-activation** output which would be passed to the activation function to obtain the transformed output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af3145-6c45-4d40-b2bc-d87953b40b91",
   "metadata": {},
   "source": [
    "1. activation sigmoid -\n",
    "    - for binary\n",
    "\n",
    "Let's say we are trying to see if an animal is mammal or not, we have three features limbs, eggs, hair, each features goes through linear layers and obtain a number let' say 6\n",
    "- we take the pre-activation output (6) and pass it to sigmoid function\n",
    "- it obtans a value between 0 and 1\n",
    "- if output is > 0.5, class label = 1 (mammal)\n",
    "- if output is <= 0.5, class label = 0 (not mammal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06a63741-bf5f-48f0-86d2-50cbcc0e4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f6b0dc5-158b-4265-a6cd-8ed8c2ed072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9975]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[6]])\n",
    "sigmoid = nn.Sigmoid() # takes one dimensional input tensor\n",
    "output = sigmoid(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af179de-7ea0-4226-b051-566dde3985a7",
   "metadata": {},
   "source": [
    "one dimensional output - bounded between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8836a1df-7061-430e-987e-51280b611a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding activation as the last layer\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6,4), # first linear layer\n",
    "    nn.Linear(4,1), #Â second linear layer\n",
    "    nn.Sigmoid() # sigmoid activation function - automating transforming the output of final layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f654727-ca83-4bdd-8cf7-ae2e6e4b17c3",
   "metadata": {},
   "source": [
    "- sigmoid as last step in network of linear layers is equivalent to traditional logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac771b-8c98-4c54-9bbc-16ada8f515e9",
   "metadata": {},
   "source": [
    "2. softmax - multi-class classification \n",
    "- takes three-dimesional as input and outputs the same shape\n",
    "- Let's say we have three classes - bird (0), mammal(1), reptile(2)\n",
    "    - in this case softmax would take three dimensional as input and outputs the same shape\n",
    "    - output is a probability distribution\n",
    "        - each element is a probability (it's bounded between 0 and 1)\n",
    "        - the sum of the output vector is equals to 1\n",
    "        - pick up the highest probability\n",
    "- similar to sigmoid, softmax could be added as the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58800154-79f4-4d13-8608-333e30cdc49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1392, 0.8420, 0.0188]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[4.3,6.1,2.3]])\n",
    "\n",
    "# apply softmax along the last dimension \n",
    "probabilities = nn.Softmax(dim=-1) # applying softmax to last dimension \n",
    "output_tensor = probabilities(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f870c32-1a44-4cd9-a59d-ab08a289242e",
   "metadata": {},
   "source": [
    "### 4. Running a Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef1ba4-d899-4f61-bfc4-df5f82d4e315",
   "metadata": {},
   "source": [
    "- Input data flows through layers\n",
    "- Calculations performed at each layer transforms the data into new representation at every layer which is passed to the next layer until the final output is produced\n",
    "- The purpose of the forward pass is to pass input data through the network and produce predictions or output based on the model learn parameters also known as weights and biases\n",
    "- this process is essential for training and making new predictions\n",
    "- Possible outcome -\n",
    "    - binary classification - single probability betwwen 0 and 1 (0.5 threshold) \n",
    "    - multi-class classification \n",
    "    - regressions - predict continous numerical values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45175192-b02c-4e8f-acc3-40945ad16786",
   "metadata": {},
   "source": [
    "### 5. Updating loss function to assess model predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea1ef3-0d7d-4484-b2c9-e8893b11ce73",
   "metadata": {},
   "source": [
    "- tells us how good our model is during training\n",
    "- takes a model prediction y-hat and ground truth y\n",
    "- input --> loss function --> output\n",
    "- input = (model prediction, ground truth), output = (float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2254a316-a51c-4bab-8e65-e02965b93be9",
   "metadata": {},
   "source": [
    "for example -  if we take our previous example \n",
    "- class 0 : mammal, class 1 : bird, class 2 : reptile\n",
    "- model prediction -\n",
    "    - predicted class = 0, correct = low loss\n",
    "    - predicted class = 1, wrong = high loss\n",
    "    - predicted class = 2, wrong = high loss\n",
    "- our goal is minimize loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282dc9e4-f364-419b-ba36-6d85e25607fc",
   "metadata": {},
   "source": [
    "- loss is calculated using a loss function f \n",
    "- **loss = F(y,y-hat)**\n",
    "    - y is a single integer (class label)\n",
    "        - eg., y = 0 when y is a mammal\n",
    "    - y-hat is a tensor (prediction before softmax)\n",
    "        - if N is the number of classes, eg., N=3\n",
    "        - y-hat is a tensor with N dimensions\n",
    "            eg., y-hat = [-5.2,4.6,0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d67663-569f-436f-bca7-6280d4dc4e10",
   "metadata": {},
   "source": [
    "**one-hot encoding** concepts -\n",
    "1. convert an integer y to a tensor of zeros and ones\n",
    "2. example - if y = 0 with three classes the encoded form is 1,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d862c35-c6d5-4ab5-b35d-0758ea8460c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# transforming labels with one-hot encoding\n",
    "import torch.nn.functional as F\n",
    "print(F.one_hot(torch.tensor(0), num_classes = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f09c710-5471-47fc-bc33-cdfe180dd552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(F.one_hot(torch.tensor(1), num_classes = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ade4150-fcdb-41d8-830e-2702ce1f77bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(F.one_hot(torch.tensor(2), num_classes = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645419f9-d72d-44ff-85f2-d00fa54e033d",
   "metadata": {},
   "source": [
    "**cross entropy loss in PyTorch** - most commanly used loss function for classification \n",
    "- after encoding we pass our prediction y-hat to a loss function\n",
    "- here y-hat is stored as tensor scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8420c8b1-7b22-4e61-b2ec-2164c01bca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8222, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "scores = torch.tensor([-5.2,4.6,0.8])\n",
    "one_hot_target = torch.tensor([1,0,0])\n",
    "criterion = CrossEntropyLoss()\n",
    "print(criterion(scores.double(),one_hot_target.double())) # ouput is the computed loss value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e31d9-ad20-4250-8037-6c8aa92757b5",
   "metadata": {},
   "source": [
    "summary - \n",
    "1. loss function takes :\n",
    "    - **scores** - model predictions before the softmax function\n",
    "    - **one_hot_target** - one hot encoded ground truth label\n",
    "2. loss function outputs :\n",
    "    - loss - a single float\n",
    "    - our goal is to minimize this loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af959ac6-f390-4e3f-8e2e-ef672b8c6360",
   "metadata": {},
   "source": [
    "### 5. Minimizing Loss - using derivatives to update model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d0000-42f3-440f-9716-e9d66d1e8e85",
   "metadata": {},
   "source": [
    "- we can use derviatives or gradient to minimise this loss\n",
    "- **derivatives** represents the slope of the curve\n",
    "    - steep slopes - large steps, derviatives is high\n",
    "    - gentler steps - small steps, derivative is low\n",
    "    - floor - flat, derivative is zero (we aim to reach this)\n",
    "- **convex and non-convex functions**\n",
    "    - convex functions have one global minimum\n",
    "    - non-convex functions have more than one global minium - values are lower than nearby points but not the lowest overall\n",
    "    - when minimizing loss functions we aim to locate the global minium where x=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046139f-b822-4778-8c31-26d5675e1357",
   "metadata": {},
   "source": [
    "connecting derivatives and model training\n",
    "- during training we run a forward pass on the features and compute loss by comparing predictions to the target values\n",
    "- compute the loss in forward pass during training\n",
    "- recall, weights and biases are randomly assigned when a model is created, we update them during the training using backward pass or **backpropogation**\n",
    "- In deep learning, derivatives are known as gradients. We compute the loss function gradients and use them to update the models parameters including weights and biases using backpropogation, repeating until the layers are tuned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed06afe-2355-4545-a77d-f2e3764adfeb",
   "metadata": {},
   "source": [
    "**Backpropogation** : \n",
    "- consider a network with three linear layers, we can calculate local loss layer with respect to each layer parameters.\n",
    "  - begin with loss gradients for l2\n",
    "  - use l2 to compute l1 gradients\n",
    "  - repeat for all layers (l1,l0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e133d-29cc-488f-b7d0-a2754a9aa194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropogation in pytorch\n",
    "\n",
    "# run a forward pass\n",
    "model = nn.Sequential(nn.Linear(16,8),\n",
    "                      nn.Linear(8,4),\n",
    "                      nn.Linear(4,2))\n",
    "prediction = model(sample)\n",
    "\n",
    "# calculate loss and gradients\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(prediction,target)\n",
    "loss.backward() # calculate gradient based on this loss stored in .grad attributes of each layers weights and biases\n",
    "# each layer in the model can be indexed starting from 0 to access it's weights, biases and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5c2ef-df84-4111-b1c2-4c6c24168fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating model parameters manually\n",
    "# 1. access each layer gradient\n",
    "#learning rate is typically small \n",
    "lr = 0.001\n",
    "# update the weights\n",
    "weight = model[0].weight\n",
    "# access each layer gradient\n",
    "weight_grad = model[0].weight.grad\n",
    "# multiply the learning rate and subtract this product from the weight\n",
    "weight = weight - lr * weight_grad\n",
    "# update the biases\n",
    "bias = model[0].bias\n",
    "bias_grad = model[0].bias.grad\n",
    "bias = bias - lr * bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931d454-c87f-478c-82cb-cd6262db47ea",
   "metadata": {},
   "source": [
    "gradient descent - find global minimum of loss functions\n",
    "1. for non-convex functions, we will use gradient descent\n",
    "2. pytorch simplifies this with optimizers\n",
    "    - stochastic gradient descent (SGD) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63864087-fd19-4605-8f88-832d8d594a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "#Â create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "#Â perform parameter updates\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548f207-3276-4039-8ec5-5a1b5523c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0de8350a-a344-44c8-a109-65f3131e8ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'animal_name': ['sparrow', 'eagle', 'cat', 'dog', 'lizard'],\n",
    "    'hair': [0, 0, 1, 1, 0],\n",
    "    'feathers': [1, 1, 0, 0, 0],\n",
    "    'eggs': [1, 1, 0, 0, 1],\n",
    "    'milk': [0, 0, 1, 1, 0],\n",
    "    'predator': [0, 1, 1, 0, 1],\n",
    "    'legs': [2, 2, 4, 4, 4],\n",
    "    'tail': [1, 1, 1, 1, 1],\n",
    "    'type': [0, 0, 1, 1, 2] # type categories - bird(0), mammal(1), repltile(2)\n",
    "}\n",
    "\n",
    "animals = pd.DataFrame(data)\n",
    "animals.to_csv('animal_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35179f0f-183d-4896-9e14-9238316086a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal_name</th>\n",
       "      <th>hair</th>\n",
       "      <th>feathers</th>\n",
       "      <th>eggs</th>\n",
       "      <th>milk</th>\n",
       "      <th>predator</th>\n",
       "      <th>legs</th>\n",
       "      <th>tail</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sparrow</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eagle</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lizard</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  animal_name  hair  feathers  eggs  milk  predator  legs  tail  type\n",
       "0     sparrow     0         1     1     0         0     2     1     0\n",
       "1       eagle     0         1     1     0         1     2     1     0\n",
       "2         cat     1         0     0     1         1     4     1     1\n",
       "3         dog     1         0     0     1         0     4     1     1\n",
       "4      lizard     0         0     1     0         1     4     1     2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ad2b0a8-b260-40a5-85c1-3063303957c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 2 1]\n",
      " [0 1 1 0 1 2 1]\n",
      " [1 0 0 1 1 4 1]\n",
      " [1 0 0 1 0 4 1]\n",
      " [0 0 1 0 1 4 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# define input features\n",
    "features = animals.iloc[:,1:-1]\n",
    "X = features.to_numpy()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f7765fa-0c6c-4729-8da6-cdb3cad59086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hair</th>\n",
       "      <th>feathers</th>\n",
       "      <th>eggs</th>\n",
       "      <th>milk</th>\n",
       "      <th>predator</th>\n",
       "      <th>legs</th>\n",
       "      <th>tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hair  feathers  eggs  milk  predator  legs  tail\n",
       "0     0         1     1     0         0     2     1\n",
       "1     0         1     1     0         1     2     1\n",
       "2     1         0     0     1         1     4     1\n",
       "3     1         0     0     1         0     4     1\n",
       "4     0         0     1     0         1     4     1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb36b073-b53a-4abe-b0d9-42fb0dd13d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# define target values (ground truth)\n",
    "target = animals.iloc[:,-1]\n",
    "y = target.to_numpy()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbc8fdef-7558-4145-81b9-29b4c15e82fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sample: tensor([0, 1, 1, 0, 0, 2, 1])\n",
      "label sample: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "# instantiate dataset class\n",
    "# torch.tensor() - coverts your feature matrix / labels into a tensor\n",
    "dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "# access to individual sample - first sample in the dataset\n",
    "input_sample, label_sample = dataset[0]\n",
    "print('input sample:', input_sample)\n",
    "print('label sample:', label_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f651f1dd-213d-4aba-a136-572443a7e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader - manage data loading\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 2 # batches to be included in each iteration \n",
    "shuffle = True # randomizes the data order at each epoch helps improve model generalisation  \n",
    "# create a data loader\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4de995-5779-4413-a2b8-7cf57233970e",
   "metadata": {},
   "source": [
    "- epoch : one full pass through the training dataloader\n",
    "- generalization : model performs well with unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31f53b2c-5b78-4aa2-b048-36e1c2506c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_inputs: tensor([[1, 0, 0, 1, 0, 4, 1],\n",
      "        [0, 1, 1, 0, 0, 2, 1]])\n",
      "batch_labels: tensor([1, 0])\n",
      "batch_inputs: tensor([[0, 1, 1, 0, 1, 2, 1],\n",
      "        [1, 0, 0, 1, 1, 4, 1]])\n",
      "batch_labels: tensor([0, 1])\n",
      "batch_inputs: tensor([[0, 0, 1, 0, 1, 4, 1]])\n",
      "batch_labels: tensor([2])\n"
     ]
    }
   ],
   "source": [
    "# interating through dataloader\n",
    "#Â each element in dataloader is a tuple which we unpack as batch_labels and batch_inputs\n",
    "for batch_inputs, batch_labels in dataloader : \n",
    "    print('batch_inputs:', batch_inputs)\n",
    "    print('batch_labels:',batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5b4b9-275b-45ac-94b5-648619f3ff00",
   "metadata": {},
   "source": [
    "since our dataset contains 5 animals and we set batch size = 2, the first iteration randomely selects two animals and their corresponding labels and so on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e2609-af60-4351-bba6-6b772fbcc0b5",
   "metadata": {},
   "source": [
    "## Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086a314-b9d6-4f62-b566-e55d8a986c02",
   "metadata": {},
   "source": [
    "writing our first training loop\n",
    "1. create a model\n",
    "2. choose a loss function\n",
    "3. define a dataset\n",
    "4. set an optimizer\n",
    "5. run a training loop\n",
    "    1. calculate loss (forward pass)\n",
    "    2. compute gradients (backpropogation)\n",
    "    3. updating model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6eb4604b-1008-4bac-a012-a85eb1d5b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset - data scientist salary \n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'experience_level': [0, 1, 2, 1, 2],\n",
    "    'employment_type': [0, 0, 0, 0, 0],\n",
    "    'remote_ratio': [0.5, 1.0, 0.0, 1.0, 1.0],\n",
    "    'company_size': [1, 2, 1, 0, 1],\n",
    "    'salary_in_usd': [0.036, 0.133, 0.234, 0.076, 0.170]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('data_science_salaries.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d01446-d62c-4204-9c1f-5eae7eec8916",
   "metadata": {},
   "source": [
    "our dataset - \n",
    "- features : categorical, target : salary(USD)\n",
    "- since the target is continous - this is a regression problem\n",
    "- for regression we will use a linear layer as final output instead of sigmoid or softmax\n",
    "- we will also apply a regression-specific loss function as cross entropy is only use for classification task\n",
    "- we will use mean squared error loss (mse) as loss function for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d65f021-2b84-4894-84e7-aee0ea43f427",
   "metadata": {},
   "source": [
    "**Mean Squared Error Loss (MSE)**\n",
    "- MSE loss is the mean of the squared difference between predictions and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f534bb-8b8c-4eb9-9943-96a465dc4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error loss (mse) - python version \n",
    "def mean_squared_loss(prediction, target):\n",
    "    return np.mean((prediction - target)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b59b7-9384-4c20-bb62-d7ea10d1002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error loss (mse) - pytorch version\n",
    "criterion = nn.MSEloss()\n",
    "# prediction and target are float tensors\n",
    "loss = criterion(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada06c3-22af-4027-9131-150c2e506006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting everything together \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# we have two numpy arrays - features and target containing our data and labels\n",
    "# we start by passing this to tensor dataset to organise our features and targets into the right data types\n",
    "# .float() datatype is used by the parameter of our model\n",
    "features = df.iloc[:,:-1].to_numpy() # all columns except last\n",
    "target = df.iloc[:,-1].to_numpy() # last column\n",
    "dataset = TensorDataset(torch.tensor(features).float(),\n",
    "                        torch.tensor(target).float())\n",
    "# load the dataset \n",
    "dataloader = DataLoader(dataset,batch_size=4,shuffle=True)\n",
    "# create the model\n",
    "# dataset has 4 input features and 1 target output\n",
    "# we won't need one hot encoding as this is a regression problem\n",
    "model = nn.Sequential(nn.Linear(4,2),\n",
    "                      nn.Linear(2,1))\n",
    "# create the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7216d3b-c174-4d48-b33f-1597ef88a303",
   "metadata": {},
   "source": [
    "In scikit-learn, the training loop is wrapped in the .fit() method, while in PyTorch, it's set up manually. While this adds flexibility, it requires a custom implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9adc1-a71d-4c6d-8015-81bf48dbd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training loop - looping through the entire dataset once is called an 'epoch'\n",
    "#Â we train over mulitple epochs\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader: # for each epoch for loop through a dataloader \n",
    "        # each iteration of the dataloader provides a batch of samples\n",
    "        # set the gradients to zero \n",
    "        optimizer.zero_grad() # optimizers stores gradient from previous steps by default\n",
    "        # get feature and target from the dataloader \n",
    "        feature, target = data # feature for forward pass and target for loss calculation\n",
    "        # run a forward pass \n",
    "        pred = model(feature)\n",
    "        # compute loss and gradients\n",
    "        loss = criterion(pred,target)\n",
    "        loss.backward()\n",
    "        # update the parameters\n",
    "        optimizer.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f2f79-aa86-4c2a-a20e-f29b7a59ab0c",
   "metadata": {},
   "source": [
    "## ReLU activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc143b-e6b8-4526-964f-68d49da03a81",
   "metadata": {},
   "source": [
    "- some activation functions can shrink gradient too much making it inefficient\n",
    "- so far we have worked with sigmoid and softmax.\n",
    "\n",
    "1. **limitations of sigmoid function** - used for binary classification\n",
    "    - outputs bounded between 0 and 1, it is usable anywhere in a network\n",
    "    - but the gradients are very small for large and small values of x\n",
    "    - this causes saturation, leading to the vanishing gradient problems - during backpropogation this becomes problematic because each gradient depends on the previous one, when gradients are really small they fail to update the weights effectively known as **vanishing gradients problem** and it makes training deep network very difficult\n",
    "\n",
    "2. **limitations of softmax function** - used for multi-class classification\n",
    "    - outputs bounded between 0 and 1\n",
    "    - have same saturation issue\n",
    "\n",
    "therefore, both of these activation functions are not ideall for hidden layers and are best used in last layers only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f758da-2ac1-4adb-b416-5fa154fd2e7b",
   "metadata": {},
   "source": [
    "**Rectified Linear Unit (ReLU)**\n",
    "- outputs maximum value between it's input and 0 - *f(x) = max(x,0)*\n",
    "- for positive inputs : outputs equals input\n",
    "- for negative inputs : output is 0\n",
    "- this function has no upper bound\n",
    "- gradients do not approach 0 for large values of x which helps overcome vaishing gradients problem\n",
    "- in pytorch - *relu = nn.ReLU()*\n",
    "- it's reliable activation function for many deep learning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797a3f4-269d-40b4-8628-c768d1072aba",
   "metadata": {},
   "source": [
    "**Leaky ReLU**\n",
    "- variation of ReLU function.\n",
    "- for positive inputs it behaves exactly like ReLU\n",
    "- for negative inputs - scaled by a small coefficient (default 0.01) for pytorch\n",
    "- this ensures the gradients for negative inputs are non-zero - preventing neurons to completly stop learning which can happen with standard value.\n",
    "- In pytorch :\n",
    "    leaky_relu = nn.LeakyReLU(neagtive_slope = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0926b5-ce05-4445-86c3-0fcc81bbc201",
   "metadata": {},
   "source": [
    "## Learning rate and momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d76515-1198-45dd-8d21-19b60f09f940",
   "metadata": {},
   "source": [
    "- training a neural network = solving an optimization problem by minimizing the loss function and adjusting the parameters\n",
    "- to do this we use an algorithm called **Stochastic Gradient Descent (SGD)** optimizer\n",
    "- optimizer we used to final global minimum of loss functions\n",
    "- the optimizer takes the model parameter with two key arguments -\n",
    "    - **learning rate** : controls the step size of updates\n",
    "    - **momentum** : adds intertia to avoid getting stuck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c134898-f02c-4a05-8e9c-a6d4ab178c15",
   "metadata": {},
   "source": [
    "for example we run our optimizer on a non-convex function - \n",
    "- lr = 0.01, momentum = 0, after 100 steps mimium found for x = -1.23 and y = -0.14\n",
    "- it got stuck in first dip of the function (multiple u's and got stuck in the first) which is not it's global minimum\n",
    "- lr = 0.01 and momentum = 0.9 we can find minimum - steps were large if previous steps were large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c44314-0416-4d43-9144-461245c969b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent \n",
    "sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71178b6-4c79-47cf-bb1c-4489e2d37be2",
   "metadata": {},
   "source": [
    "**summary**\n",
    "\n",
    "| **Learning Rate**                               | **Momentum**                     |\n",
    "| ----------------------------------------------- | -------------------------------- |\n",
    "| Controls the step size                          | Controls the inertia             |\n",
    "| Too high â poor performance                     | Helps escape local minimum       |\n",
    "| Too low â slow training                         | Too small â optimizer gets stuck |\n",
    "| Typical range: `0.01 (10â»Â²)` to `0.0001 (10â»â´)` | Typical range: `0.85 to 0.99`    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584f4b4-0192-47ad-a9c5-61a718686294",
   "metadata": {},
   "source": [
    "## Layer Initialization and transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79166242-6e33-4ce0-a421-165779ae8a31",
   "metadata": {},
   "source": [
    "- Data normalization scales input features for stability, similarly, the weights of the linear layers are initialized to small values also known as **layer initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e4848ca-3bad-415b-9e8d-dded1a7ba750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1250, grad_fn=<MinBackward1>) tensor(0.1250, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "layer = nn.Linear(64,128)\n",
    "print(layer.weight.min(), layer.weight.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a129a-7e40-41bc-bc30-4c7031fd5a9f",
   "metadata": {},
   "source": [
    "- the weights are between -0.1250 and 0.1250.\n",
    "- output of neuron in a linear layer is a weighted sum of input from the previous layer\n",
    "- keeping both the input data and layer weights small ensures stable output, preventing extreme values that can slow training\n",
    "- layers can be initalised in different ways (active area of research)\n",
    "- pytorch provides an easy way to initialize layer weights using nn.init module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b254688c-14fe-42db-a7c3-7e8b687b672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0006, grad_fn=<MinBackward1>) tensor(0.9998, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "layer = nn.Linear(64,128)\n",
    "nn.init.uniform_(layer.weight)\n",
    "print(layer.weight.min(), layer.weight.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2588b5f-29fe-4bc6-97df-9810ca7be08f",
   "metadata": {},
   "source": [
    "here we have initialized layer with uniform distribution, the weight values ranges between 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f3cda-0d3a-40b4-b9a5-552da6ee7abe",
   "metadata": {},
   "source": [
    "**Transfer Learning** - \n",
    "- takes the model that was used in first task and re-use it for second similar task\n",
    "- for example - we trained a model on US data scientist salaries, now instead of using another model with randomely initialised weights we can load weights from the first model and use them as starting point to train this new dataset\n",
    "- torch.save() used for saving weights from previous model\n",
    "- torch.load() loading the weights from previous model\n",
    "- the functions save and load works with any kind of pytorch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38e2a2-c9b3-4afe-a33d-0fe28cb3cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "layer = nn.Linear(64,128)\n",
    "torch.save(layer,'layer.pth')\n",
    "new_layer = torch.load('layer.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99db85-e205-42c7-aaaa-5d66e77f0471",
   "metadata": {},
   "source": [
    "**Fine Tuning** \n",
    "- a type of transfer learning\n",
    "- load weights from previously trained model but train the model with smaller learning rate\n",
    "- we can even train part of the network (we freeze some of them)\n",
    "- rule of thumb : freeze early layers of network and fine-tune layers closer to output layer\n",
    "- this could be done by setting each parameters requires_grad attribute to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8d2e9-ebb6-4255-91eb-971ff6be4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Sequential(nn.Linear(64,128),\n",
    "                      nn.Linear(128,256))\n",
    "for name, param in model.named_parameters():\n",
    "    if name == '0.weight':\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db71e57-01c1-4ad4-9a56-de88f5dd689d",
   "metadata": {},
   "source": [
    "in this case we have used model.named_parameters() method, which returns the name and parameter itself and we set requires_grad for first layer to False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f0657-5fe5-4584-bd93-b32ca57d625d",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97383b-b896-4a36-8f78-b337ed161d28",
   "metadata": {},
   "source": [
    "- A dataset is typically split into three subsets - \n",
    "    1. Training - 80-90% data - adjust model parameters (weights and biases) \n",
    "    2. Validation - 10-20% data - tunes hyperparameters (learning rate and momentum) \n",
    "    3. Test - 5-10% - evaluates final model performance\n",
    "- Track loss and accuracy during training and validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de5c66-0cf9-4a81-b1e3-4cabee1da0ae",
   "metadata": {},
   "source": [
    "**Calculating Training loss** \n",
    "for each epoch : \n",
    "- sum the loss across all batches in the training dataloader\n",
    "- at the end of each epoch we compute the mean training loss by dividing the total loss by number of batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4344c-a09b-4008-bb59-df5ab8bc9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we begin by setting training loss to 0 \n",
    "training_loss = 0.0 \n",
    "#Â iterate through the train loader\n",
    "for inputs, labels in trainloader:\n",
    "    # run the forward pass\n",
    "    outputs = model(inputs)\n",
    "    # compute the loss\n",
    "    loss = criterion(outputs,labels)\n",
    "    # backpropogation \n",
    "    loss.backward() # compute gradients\n",
    "    optimizer.step() # update the weights\n",
    "    optimizer.zero_grad() # reset gradients\n",
    "    # calculate and sum the loss\n",
    "    training_loss += loss.item()\n",
    "    epoch_loss = training_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85896498-fd05-484f-8cfd-b89c81e89fba",
   "metadata": {},
   "source": [
    "**Calculating validation loss** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6369aa-80f7-4168-9db0-0430e8834392",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_mode = 0.0\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "# disable gradients for efficiency (Since we don't update weights during validation)\n",
    "with torch.no_grad():\n",
    "    # iterate through validation data loader\n",
    "    for inputs, labels in validationloader:\n",
    "        # run the forward pass\n",
    "        outputs = model(inputs)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "epoch_loss = validation_loss / len(validationloader) # compute mean validation loss\n",
    "# switch back to training mode - preparing it for next training epoch\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1d80a-7ab7-44da-8f72-4a536e8a1dfa",
   "metadata": {},
   "source": [
    "- keeping track of validation loss and training loss helps us keep track of overfitting\n",
    "- when a model overfits training loss keeps decreasing but validation loss starts to rise, this means the model is learning the training data too well and would not perform well in new data\n",
    "- loss tell us our model is learning but doesn't alwasy makes accurate prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b96d15-fe30-4ce0-8893-cf30cd9165f3",
   "metadata": {},
   "source": [
    "**Calculating accuracy with torchmetrics** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ce008-9ab4-4e4d-8162-09d90c60b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics \n",
    "# create accuracy metric\n",
    "metric = trochmetrics.Accuracy(task=\"multiclass\",num_classes=3)\n",
    "# as model process each batch we update this metric using it's prediction and actual label\n",
    "for features, labels in dataloader:\n",
    "    outputs = model(features) # forward pass\n",
    "    # compute batch accuracy (keeping argmax for on-hot labels)\n",
    "    # model outputs probabilities for multiple classes we use argmax(dim=-1) to select the class with highest probability \n",
    "    # this converts one-hot encoded predictions into class index before passing it to the metrics \n",
    "    metric.update(outputs, labels.argmax(dim=-1))\n",
    "\n",
    "# at end of each epoch we calculate final accuracy \n",
    "accuracy = metric.compute()\n",
    "\n",
    "# reset metric for the next epoch\n",
    "metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b1859-d06e-43bb-a940-3354bea0f637",
   "metadata": {},
   "source": [
    "### Fighting Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21ef88-ca4c-45a6-912d-074cd2d095fb",
   "metadata": {},
   "source": [
    "- **overfitting** - the model does not generalize to unseen data\n",
    "\n",
    "- to avoid overfitting\n",
    "    - reducing model size or adding dropout layer\n",
    "    - using weight decay to force parameters to remain small\n",
    "    - obtaining new data or augmenting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e79c0e-357e-4a73-8196-b89226c2f077",
   "metadata": {},
   "source": [
    "common way to avoid overfitting is to add dropout layer to our neural network - **regularization**\n",
    "- randmoley zeros out elements of the input tensor during training - preventing the model getting too dependent to specific features\n",
    "- dropout layers are added after the activation functions\n",
    "- behaves differently in training vs. evaluation - use model.train() for training and model.eval() to disable dropout during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e35ea18-517d-48c3-be2a-58ca1793c129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 1.0263, 0.0120, 0.0000]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(8,4),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(p=0.5)) # p determines the probability of the neurons set to zero\n",
    "\n",
    "features = torch.randn((1,8))\n",
    "print(model(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f7c5d-9cbc-439e-b456-6a3fb5c35490",
   "metadata": {},
   "source": [
    "next strategy to reduce overfitting is using **weight decay** another form of regualarization.\n",
    "- in pytorch - weight decay is added to the optimizer using weight_decay parameter - typically set to a very small value.\n",
    "- this parameter adds a penalty to loss function, encouraging smaller weights and helping the model generalize better \n",
    "- during back propogation this penalty is subtracted from the gradient, preventing excessive weight growth\n",
    "- the higher we set the weight decay, the stronger the regularization, making overfitting less likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d300e-6c54-4263-a559-f4e519163fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_decay using pytorch\n",
    "optimizer = optmin.SGD(model.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac5fdb4-20eb-49f0-9be2-910dd477aa70",
   "metadata": {},
   "source": [
    "**Data Augmentation**\n",
    "- collected large data could be expensive, but there is a way to expand datasets artifically using data augmentation\n",
    "- Data Augmentation is commanly applied to image data, which can be rotated and scaled so that different views of the same face become available as \"new\" data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219b00c-f04c-4645-8eec-2566e41a1656",
   "metadata": {},
   "source": [
    "### Improving Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96508005-319f-4f10-a1c5-162ec72e1e6c",
   "metadata": {},
   "source": [
    "Steps to maximize performance - \n",
    "1. **Overfit the training set**\n",
    "   - create a model that can overfit the training set, this will ensure that the problem is solvable.\n",
    "   - we also set a performance baseline to aim for the validation set \n",
    "2. **Reduce Overfitting**\n",
    "   -  we need to reduce overfitting to increase performace on the validation set\n",
    "3. **Fine-Tune the Parameters**\n",
    "    - we can slightly adjust the different hyperparamteres to ensure we achieve the best possible performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892451d-f5bf-47ad-948e-3a0c27e49140",
   "metadata": {},
   "source": [
    "**Step 1 - overfit the training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5229627-60df-4a6c-ad7a-cd5eaf3bcb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the training loop to overfit a single data point\n",
    "features, labels = next(iter(dataloader))\n",
    "for i in range(1000):\n",
    "    outputs = model(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4887541-03d6-42b0-b386-73509f127c94",
   "metadata": {},
   "source": [
    "- when the model is set up properly, it should quickly reach near-zero loss and 100% accuracy on that data point.\n",
    "- once this step is succesful, we scale up to the entire training set. At this stage, we use an existing model architecture large enough to overfit while keeoping hyperparameters like the learning rate at their defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748bc22-d688-4889-af0c-1deaa09bf8ce",
   "metadata": {},
   "source": [
    "**Step 2 - Reduce Overfitting**\n",
    "- goal : maximize the validation accuracy\n",
    "- experiment with -\n",
    "    - dropout\n",
    "    - data augmentation\n",
    "    - weight decay\n",
    "    - reducing model capacity\n",
    "- keep track of the different parameters and the corresponding validation accuracy for each set of experiments.\n",
    "- reducing overfitting often comes at a cost, as applying regularization can significantly impact model performance\n",
    "- the original model overfits the training set, achieving high accuracy but failing to generalize well to new data\n",
    "- In contrast, with too much regularization, the update model shows a drop in training and validation accuracy, limiting its ability to learn effectively.\n",
    "- This highlights the importance of balancing overfitting reduction strategies while closely monitoring key metics to find the best performing model\n",
    "- once we are statisfied with performace, the next step is fine-tune hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f3ae4-d60b-4155-ba77-b722b6302bc6",
   "metadata": {},
   "source": [
    "**Step 3 - fine-tune hyperparameters**\n",
    "- this is often done on optimizer setting likes learning rate or momentum\n",
    "- Grid Search tests paramters at fixed intervals\n",
    "- random search - instead of testing set values, it randomely selects them withing a given range\n",
    "    - random search is more efficient as it avoids uneccesary tests and increase the chance of finding the optimial settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5886ca-ee0e-4eeb-bacc-e740c82f5958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "for factor in range(2,6):       \n",
    "    lr = 10 ** -factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218358a9-eb59-4bea-8bfe-1273ca97794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random search\n",
    "factor = np.random.uniform(2,6)\n",
    "lr = 10 ** -factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdf5f9-3d02-443a-8069-7f7a6e3a8e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501028a-362a-425b-a76a-49326d0e8be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a8fb3-2a37-4c85-bbdb-cba0bdff4eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c67c2-87e3-4617-8692-7e42229003b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
